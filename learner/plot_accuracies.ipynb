{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from statistics import mean, stdev\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_base_path = \"./store_with_leq_topk10\"\n",
    "result_base_path = \"./store/dataset=airport/use_val_data=True/dirty-proportion=0.23/trainer-prior-type=uniform-0.1-learner-prior-type=uniform-0.9\"\n",
    "\n",
    "folders = os.listdir(result_base_path)\n",
    "print(len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict((trainer_type, {\n",
    "    'Random': {'iter_accuracy': [],\n",
    "               'iter_recall': [],\n",
    "               'iter_precision': [],\n",
    "               'iter_f1': [],\n",
    "               'iter_mae_ground_model_error': [],\n",
    "               'iter_mae_trainer_model_error': []\n",
    "               },\n",
    "    'ActiveLR': {'iter_accuracy': [],\n",
    "                 'iter_recall': [],\n",
    "                 'iter_precision': [],\n",
    "                 'iter_f1': [],\n",
    "                 'iter_mae_ground_model_error': [],\n",
    "                 'iter_mae_trainer_model_error': []\n",
    "                 },\n",
    "    'StochasticBR': {'iter_accuracy': [],\n",
    "                     'iter_recall': [],\n",
    "                     'iter_precision': [],\n",
    "                     'iter_f1': [],\n",
    "                     'iter_mae_ground_model_error': [],\n",
    "                     'iter_mae_trainer_model_error': []\n",
    "                     },\n",
    "    'StochasticUS': {'iter_accuracy': [],\n",
    "                     'iter_recall': [],\n",
    "                     'iter_precision': [],\n",
    "                     'iter_f1': [],\n",
    "                     'iter_mae_ground_model_error': [],\n",
    "                     'iter_mae_trainer_model_error': []\n",
    "                     }\n",
    "}) for trainer_type in ['full-oracle', 'learning-oracle', 'bayesian'])\n",
    "\n",
    "for folder in folders:\n",
    "    trainer_type, sampling_method, _ = folder.split(\"_\")\n",
    "    with open(os.path.join(result_base_path, folder, \"study_metrics.json\"), 'r') as fp:\n",
    "        study_metrics = json.load(fp)\n",
    "        for metric_type in ['iter_accuracy', 'iter_recall', 'iter_precision', 'iter_f1',     'iter_mae_ground_model_error', 'iter_mae_trainer_model_error']:\n",
    "            folder_name = None\n",
    "            if sampling_method == 'RANDOM':\n",
    "                folder_name = 'Random'\n",
    "            elif sampling_method == 'ACTIVELR':\n",
    "                folder_name = 'ActiveLR'\n",
    "            elif sampling_method == 'STOCHASTICBR':\n",
    "                folder_name = 'StochasticBR'\n",
    "            elif sampling_method == 'STOCHASTICUS':\n",
    "                folder_name = 'StochasticUS'\n",
    "            results_dict[trainer_type][folder_name][metric_type].append([list(range(1, len(\n",
    "                study_metrics[metric_type])+1)), study_metrics['elapsed_time'], study_metrics[metric_type]])\n",
    "\n",
    "# pprint(results_dict['uninformed-bayesian']['ActiveLR']['iter_mae_ground_model_error'][0][2])\n",
    "# pprint(results_dict['uninformed-bayesian']['ActiveLR']['iter_mae_trainer_model_error'][0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Average time and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For Random'''\n",
    "average_dict = dict((trainer_type, {\n",
    "    'Random': {'iter_accuracy': [],\n",
    "               'iter_recall': [],\n",
    "               'iter_precision': [],\n",
    "               'iter_f1': [],\n",
    "               'iter_mae_ground_model_error': [],\n",
    "               'iter_mae_trainer_model_error': []\n",
    "               },\n",
    "    'ActiveLR': {'iter_accuracy': [],\n",
    "                 'iter_recall': [],\n",
    "                 'iter_precision': [],\n",
    "                 'iter_f1': [],\n",
    "                 'iter_mae_ground_model_error': [],\n",
    "                 'iter_mae_trainer_model_error': []\n",
    "                 },\n",
    "    'StochasticBR': {'iter_accuracy': [],\n",
    "                     'iter_recall': [],\n",
    "                     'iter_precision': [],\n",
    "                     'iter_f1': [],\n",
    "                     'iter_mae_ground_model_error': [],\n",
    "                     'iter_mae_trainer_model_error': []\n",
    "                     },\n",
    "    'StochasticUS': {'iter_accuracy': [],\n",
    "                     'iter_recall': [],\n",
    "                     'iter_precision': [],\n",
    "                     'iter_f1': [],\n",
    "                     'iter_mae_ground_model_error': [],\n",
    "                     'iter_mae_trainer_model_error': []\n",
    "                     }\n",
    "}) for trainer_type in ['full-oracle', 'learning-oracle', 'bayesian'])\n",
    "\n",
    "for trainer_type in ['full-oracle', 'learning-oracle', 'bayesian']:\n",
    "    for sampling_type in ['Random', 'ActiveLR', 'StochasticBR', 'StochasticUS']:\n",
    "        for metric_type in ['iter_accuracy', 'iter_recall', 'iter_precision', 'iter_f1', 'iter_mae_ground_model_error', 'iter_mae_trainer_model_error']:\n",
    "            for exp_metrics_lst in zip(*results_dict[trainer_type][sampling_type][metric_type]):\n",
    "                average_lst = []\n",
    "                # print(exp_metrics_lst)\n",
    "\n",
    "                max_len = max(len(exp_metric)\n",
    "                              for exp_metric in exp_metrics_lst)\n",
    "                # print(max_len)\n",
    "                for idx in range(max_len):\n",
    "                    candidate_lst = [exp_metric[idx] for exp_metric in exp_metrics_lst if idx < len(\n",
    "                        exp_metric) and str(exp_metric[idx]) != 'nan']\n",
    "                    if candidate_lst:\n",
    "                        average_lst.append(mean(candidate_lst))\n",
    "                    else:\n",
    "                        average_lst.append(np.nan)\n",
    "                average_dict[trainer_type][sampling_type][metric_type].append(\n",
    "                    average_lst)\n",
    "        # print(average_dict[sampling_type][metric_type])\n",
    "# print(average_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''For Random'''\n",
    "# average_dict = {'Random': {\n",
    "#     'iter_accuracy': [],\n",
    "#     'iter_recall': [],\n",
    "#     'iter_precision': [],\n",
    "#     'iter_f1': []\n",
    "# },\n",
    "#     'ActiveLR': {\n",
    "#     'iter_accuracy': [],\n",
    "#     'iter_recall': [],\n",
    "#     'iter_precision': [],\n",
    "#     'iter_f1': []\n",
    "# }\n",
    "# }\n",
    "# for sampling_type in ['Random', 'ActiveLR']:\n",
    "#     for metric_type in ['iter_accuracy', 'iter_recall', 'iter_precision', 'iter_f1']:\n",
    "#         for val in zip(*results_dict[sampling_type][metric_type]):\n",
    "#             idx = 0\n",
    "#             average_vals = []\n",
    "#             candidate_vals = []\n",
    "#             end = False\n",
    "#             while not end:\n",
    "#                 end = True\n",
    "#                 for exp_lst in val:\n",
    "#                     if idx < len(exp_lst) and (str(exp_lst[idx]) != 'nan'):\n",
    "#                         '''If any found then end is false'''\n",
    "#                         candidate_vals.append(exp_lst[idx])\n",
    "#                         end = False\n",
    "#                 else:\n",
    "#                     idx += 1\n",
    "#                     # print(mean(candidate_vals), candidate_vals)\n",
    "#                     print(candidate_vals)\n",
    "#                     average_vals.append(mean(candidate_vals))\n",
    "\n",
    "#             average_dict[sampling_type][metric_type].append(average_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(average_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracy', 'recall', 'precision', 'f1', 'mae_ground_model_error', 'mae_trainer_model_error']:\n",
    "# for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    for trainer_type in ['bayesian']:\n",
    "    # for trainer_type in ['full-oracle', 'learning-oracle', 'bayesian']:\n",
    "    # for metric in ['accuracy','mae_model_error']:\n",
    "        plt.figure(figsize=(12,7))\n",
    "        # plt.subplot(2,1,1)\n",
    "        for sampling_method in average_dict[trainer_type]:\n",
    "            plt.plot(average_dict[trainer_type][sampling_method][f'iter_{metric}'][0], average_dict[trainer_type][sampling_method][f'iter_{metric}'][2], label=sampling_method)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel(metric)\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.legend()\n",
    "        # plt.subplot(2,1,2)\n",
    "        # for sampling_method in average_dict[trainer_type]:\n",
    "        #     plt.plot(average_dict[trainer_type][sampling_method][f'iter_{metric}'][1], average_dict[trainer_type][sampling_method][f'iter_{metric}'][2], label=sampling_method)\n",
    "        # plt.xlabel('Time')\n",
    "        # plt.ylabel(metric)\n",
    "        # plt.legend()\n",
    "        plt.title(f\"Clean/Dirty Prediction {metric} from Learner Model with {trainer_type} trainer\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
