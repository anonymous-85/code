{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import pickle as pk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/raw-data/omdb-clean-full.csv\"\n",
    "\n",
    "raw_df = pd.read_csv(data_path)\n",
    "raw_df.rename(columns=dict((col, col.lower()) for col in raw_df.columns), inplace=True)\n",
    "raw_df.index = raw_df.index.map(str)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Read Scenario3 info for the functional dependencies\n",
    "\n",
    "with open(\"./scenarios.json\", 'r') as fp:\n",
    "    scenarios = json.load(fp)\n",
    "required_scenario_info = scenarios[\"3\"]\n",
    "hypothesis_space = [hypothesis['cfd'] for hypothesis in required_scenario_info['hypothesis_space']]\n",
    "print(hypothesis_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add info to new scenarios dict and dump the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hypothesis(fd):\n",
    "    lfd, rfd = fd.split(\"=>\")\n",
    "\n",
    "    '''Parse left fd and separate out the attributes'''\n",
    "    left_attributes = lfd.strip().strip(\"(\").strip(\")\").split(\",\")\n",
    "    right_attributes = rfd.strip(\"(\").strip(\")\").split(\",\")\n",
    "\n",
    "    left_attributes = [attribute.strip() for attribute in left_attributes]\n",
    "    right_attributes = [attribute.strip() for attribute in right_attributes]\n",
    "\n",
    "    return left_attributes, right_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_support_violation(fd_components, tuple_1, tuple_2):\n",
    "    '''Parse the hypothesis'''\n",
    "    lfd, rfd = fd_components\n",
    "\n",
    "    '''Violation check is only needed if the lfd values are same in both tuples otherwise it's not a violation'''\n",
    "    is_left_same = all(tuple_1[left_attribute] == tuple_2[left_attribute] for left_attribute in lfd)\n",
    "\n",
    "    if is_left_same:\n",
    "        is_right_same = all(tuple_1[left_attribute] == tuple_2[left_attribute] for left_attribute in rfd)\n",
    "        if is_right_same:\n",
    "            return True, False\n",
    "        else:\n",
    "            return False, True\n",
    "    else:\n",
    "        return False, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_violation_tuples(data, idx, fd_components):\n",
    "    supports = []\n",
    "    violations = []\n",
    "    for idx_ in data.index:\n",
    "        if idx == idx_:\n",
    "            continue\n",
    "        is_support, is_violation = is_support_violation(fd_components=fd_components, tuple_1=data.iloc[idx], tuple_2=data.iloc[idx_])\n",
    "        if is_support:\n",
    "            supports.append(idx_)\n",
    "        elif is_violation:\n",
    "            violations.append(idx_)\n",
    "    return supports, violations\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesis_info_dict(hypothesis):\n",
    "    '''Extract left and right attributes from the hypothesis as list of attributes'''\n",
    "    lfd, rfd = parse_hypothesis(hypothesis)\n",
    "    info_dict = {'lfd': lfd, 'rfd':rfd}\n",
    "\n",
    "    '''Find pairwise violations of each tuple with respect to other tuples in the dataset'''\n",
    "    info_dict['supports'] = dict()\n",
    "    info_dict['violations'] = dict()\n",
    "    for idx in tqdm(raw_df.index):\n",
    "        supports, violations = get_support_violation_tuples(data=raw_df[lfd+rfd], idx=idx, fd_components=(lfd, rfd))\n",
    "        info_dict['supports'][idx] = supports\n",
    "        info_dict['violations'][idx] = violations\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "# get_hypothesis_info_dict(hypothesis_space[0])\n",
    "# hypothesis_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_num = os.cpu_count()\n",
    "cpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scenarios_dict = dict()\n",
    "new_scenarios_dict['omdb'] = dict()\n",
    "new_scenarios_dict['omdb']['hypothesis_space'] = dict()\n",
    "\n",
    "\n",
    "with Pool(cpu_num) as p:\n",
    "    hypothesis_info = p.map(get_hypothesis_info_dict, hypothesis_space)\n",
    "\n",
    "for hypothesis, info_dict in zip(hypothesis_space, hypothesis_info):\n",
    "    new_scenarios_dict['omdb']['hypothesis_space'][hypothesis] = info_dict\n",
    "\n",
    "# for hypothesis in tqdm(hypothesis_space):\n",
    "    \n",
    "    # '''Extract left and right attributes from the hypothesis as list of attributes'''\n",
    "    # lfd, rfd = parse_hypothesis(hypothesis)\n",
    "    # new_scenarios_dict['omdb']['hypothesis_space'][hypothesis] = {'lfd': lfd, 'rfd':rfd}\n",
    "\n",
    "    # '''Find pairwise violations of each tuple with respect to other tuples in the dataset'''\n",
    "    # new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'] = dict()\n",
    "    # for idx in raw_df.index:\n",
    "    #     new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'][idx]=get_violation_tuples(data=raw_df, idx=idx, fd=hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in new_scenarios_dict:\n",
    "    new_scenarios_dict[dataset]['data_indices'] = [str(x) for x in raw_df.index]\n",
    "    for hypothesis in new_scenarios_dict[dataset]['hypothesis_space']:\n",
    "        for val_type in ['supports', 'violations']:\n",
    "            for idx in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type]:\n",
    "\n",
    "                if int(idx) in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx]:\n",
    "                    new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx].remove(int(idx))\n",
    "\n",
    "                '''Don't assign if the list contains self index'''\n",
    "                if new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] != []:\n",
    "                    new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] = [str(x) for x in new_scenarios_dict[dataset]['hypothesis_space'][hypothesis][val_type][idx] if str(x)!=str(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scenarios_dict['omdb']['processed_dataset_path'] = \"data/processed-data/omdb-sampled.csv\"\n",
    "new_scenarios_dict['omdb']['raw_dataset_path'] = \"data/raw-data/omdb-clean-full.csv\"\n",
    "\n",
    "with open(\"./new_scenarios.json\", 'w') as fp:\n",
    "    json.dump(new_scenarios_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./new_scenarios.json\", 'w') as fp:\n",
    "#     json.dump(new_scenarios_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read new scenarios file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_scenarios.json\", 'r') as fp:\n",
    "    new_scenarios_dict = json.load(fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_scenarios_dict['omdb']['hypothesis_space'][hypothesis_space[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_support_violation_ratio_info = dict()\n",
    "for hypothesis in new_scenarios_dict['omdb']['hypothesis_space']:\n",
    "    hypothesis_info_dict = new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]\n",
    "    if len(hypothesis_info_dict['lfd']+hypothesis_info_dict['rfd']) not in [3,4]:\n",
    "        continue\n",
    "    \n",
    "    support_pairs_num, violation_pairs_num = 0,0\n",
    "    for idx in hypothesis_info_dict['supports']:\n",
    "        support_pairs_num += len(hypothesis_info_dict['supports'][idx])\n",
    "\n",
    "    for idx in hypothesis_info_dict['violations']:\n",
    "        violation_pairs_num += len(hypothesis_info_dict['violations'][idx])\n",
    "    \n",
    "    hypothesis_support_violation_ratio_info[hypothesis] = support_pairs_num/(support_pairs_num+violation_pairs_num)\n",
    "# pprint(hypothesis_support_violation_ratio_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sample confidence from 0 to support_violation_ratio'''\n",
    "np.random.seed(1000)\n",
    "model = dict((hypothesis, np.random.uniform(max(0,ratio-0.25), min(1,ratio+0.25) )) for hypothesis, ratio in hypothesis_support_violation_ratio_info.items())\n",
    "model_dict={'omdb':{'model':model}}\n",
    "# model = dict((hypothesis, ratio ) for hypothesis, ratio in hypothesis_support_violation_ratio_info.items())\n",
    "\n",
    "# pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Predict a tuple using a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predict_clean_tuple(idx, model):\n",
    "    total_score = 0\n",
    "    for hypothesis, conf in model.items():\n",
    "        '''Check the number of supports and violations based on the model'''\n",
    "        support_pairs_num = len(new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['supports'].get(idx, []))\n",
    "        violation_pairs_num = len(new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]['violations'].get(idx, []))\n",
    "\n",
    "        '''Vote according to the hypothesis conf'''\n",
    "        total_score += (conf*(support_pairs_num-violation_pairs_num))\n",
    "    \n",
    "    if total_score > 0:\n",
    "        return True, total_score\n",
    "    \n",
    "    else:\n",
    "        return False, total_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clean_tuple_indices = set()\n",
    "model_score_dict = dict()\n",
    "for idx in new_scenarios_dict['omdb']['data_indices']:         \n",
    "    is_clean, total_score = predict_clean_tuple(idx, model)\n",
    "    if is_clean:\n",
    "        clean_tuple_indices.add(idx)\n",
    "    model_score_dict[idx] = total_score\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(clean_tuple_indices)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_dict['omdb']['predictions'] =  dict((idx, True) if idx in clean_tuple_indices else (idx,False) for idx in new_scenarios_dict['omdb']['data_indices'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.9**10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the coditional probability of a tuple being cleaned conditional to all the tuples being clean\n",
    "- Let's suppose t1 has compliance and violations with t2, t3 and t4 only. Then the conditional probability becomes independent of other variables\n",
    "- P(t1=C|t2=C,t3=C,......) = P(t1=C|t2=C,t3=C,t4=C)\n",
    "    - = P(t1=C, t2=C, t3=C, t4=C)/P(t2=C, t3=C, t4=C)\n",
    "    - = P(t1=C, t2=C, t3=C, t4=C)/(P(t1=C, t2=C, t3=C, t4=C) + P(t1=D, t2=C, t3=C, t4=C))\n",
    "    - = 1/Z\\*exp(p*(#ofcompliance(t1=C, t2=C, t3=C, t4=C)-#ofviolations_(t1=C, t2=C, t3=C, t4=C)))/[1/Z*(exp(p*(#ofcompliance(t1=C, t2=C, t3=C, t4=C)-#ofviolations_(t1=C, t2=C, t3=C, t4=C)))+(exp(p*(#ofcompliance(t1=D, t2=C, t3=C, t4=C)-#ofviolations_(t1=D, t2=C, t3=C, t4=C))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_clean_prob(idx, fd, model_probab, valid_indices = None):\n",
    "    if valid_indices is None:\n",
    "        compliance_num = len(new_scenarios_dict['omdb']['hypothesis_space'][fd]['supports'].get(str(idx), []))\n",
    "        violation_num = len(new_scenarios_dict['omdb']['hypothesis_space'][fd]['violations'].get(str(idx),[]))\n",
    "    else:\n",
    "        compliance_num = len([idx_ for idx_ in new_scenarios_dict['omdb']['hypothesis_space'][fd]['supports'].get(str(idx), []) if idx_ in valid_indices])\n",
    "        violation_num = len([idx_ for idx_ in new_scenarios_dict['omdb']['hypothesis_space'][fd]['violations'].get(str(idx),[]) if idx_ in valid_indices])\n",
    "\n",
    "    tuple_clean_score = math.exp(model_probab*(compliance_num-violation_num))\n",
    "    tuple_dirty_score = math.exp(model_probab*(-compliance_num+violation_num))\n",
    "    cond_p_clean = tuple_clean_score/(tuple_clean_score+tuple_dirty_score)\n",
    "    return cond_p_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict['omdb']['model']\n",
    "conditional_clean_probability_dict = dict()\n",
    "clean_indices = set()\n",
    "dirty_indices = set()\n",
    "\n",
    "clean_max_num = 1500\n",
    "dirty_sample_percentage = 0.1\n",
    "\n",
    "data_indices = new_scenarios_dict['omdb']['data_indices']\n",
    "\n",
    "top_10_fds = dict(sorted(model.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "\n",
    "for idx in data_indices:\n",
    "    conditional_clean_probability_dict[idx] = {'hypothesis':dict()}\n",
    "    for fd, model_probab in top_10_fds.items():\n",
    "        conditional_clean_probability_dict[idx]['hypothesis'][fd] = get_conditional_clean_prob(idx, fd, model_probab=model_probab)\n",
    "    conditional_clean_probability_dict[idx]['average'] = np.mean(list(conditional_clean_probability_dict[idx]['hypothesis'].values()))\n",
    "    is_idx_clean = conditional_clean_probability_dict[idx]['average']>=0.5\n",
    "    conditional_clean_probability_dict[idx]['is_clean'] = is_idx_clean\n",
    "\n",
    "    if is_idx_clean:\n",
    "        clean_indices.add(idx)\n",
    "    else:\n",
    "        dirty_indices.add(idx)\n",
    "else:\n",
    "    # pprint(conditional_clean_probability_dict)\n",
    "    print(len(clean_indices), len(dirty_indices))\n",
    "\n",
    "clean_sample_idxs = np.random.choice(list(clean_indices), min(len(clean_indices), clean_max_num), replace=False)\n",
    "dirty_sample_idxs = np.random.choice(list(dirty_indices), int(dirty_sample_percentage*len(clean_sample_idxs)), replace=False)\n",
    "sampled_data_indices = set(clean_sample_idxs).union(set(dirty_sample_idxs))\n",
    "print(len(sampled_data_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun the model computation and is_clean prediction using this computed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Assume every data to be clean at the beginning and compute the model based on that'''\n",
    "new_model = deepcopy(model_dict['omdb']['model'])\n",
    "model_mae = float(\"inf\")\n",
    "\n",
    "new_data_indices = sampled_data_indices\n",
    "\n",
    "while model_mae > 1e-05:\n",
    "\n",
    "    '''Use current model to predict clean and dirty indices'''\n",
    "    top_10_fds = dict(\n",
    "        sorted(new_model.items(), key=itemgetter(1), reverse=True)[:10])\n",
    "\n",
    "    new_conditional_clean_probability_dict = dict()\n",
    "    new_clean_indices = set()\n",
    "    new_dirty_indices = set()\n",
    "\n",
    "    for idx in new_data_indices:\n",
    "        new_conditional_clean_probability_dict[idx] = {'hypothesis': dict()}\n",
    "        for fd, model_probab in top_10_fds.items():\n",
    "            new_conditional_clean_probability_dict[idx]['hypothesis'][fd] = get_conditional_clean_prob(\n",
    "                idx, fd, model_probab=model_probab, valid_indices=new_data_indices)\n",
    "        new_conditional_clean_probability_dict[idx]['average'] = np.mean(\n",
    "            list(new_conditional_clean_probability_dict[idx]['hypothesis'].values()))\n",
    "        is_idx_clean = new_conditional_clean_probability_dict[idx]['average'] >= 0.5\n",
    "        new_conditional_clean_probability_dict[idx]['is_clean'] = is_idx_clean\n",
    "\n",
    "        if is_idx_clean:\n",
    "            new_clean_indices.add(idx)\n",
    "        else:\n",
    "            new_dirty_indices.add(idx)\n",
    "\n",
    "    else:\n",
    "        # pprint(new_conditional_clean_probability_dict)\n",
    "        print(f\"Clean Data Number: {len(new_clean_indices)},\"\n",
    "              f\"Dirty Data Number: {len(new_dirty_indices)},\"\n",
    "              f\"Dirty Data Proportion: {len(new_dirty_indices)/len(new_clean_indices.union(new_dirty_indices))}\")\n",
    "\n",
    "    '''Use clean data to estimate model'''\n",
    "    model_mae = 0\n",
    "    for hypothesis in new_scenarios_dict['omdb']['hypothesis_space']:\n",
    "        hypothesis_info_dict = new_scenarios_dict['omdb']['hypothesis_space'][hypothesis]\n",
    "        if len(hypothesis_info_dict['lfd']+hypothesis_info_dict['rfd']) not in [3, 4]:\n",
    "            continue\n",
    "\n",
    "        '''Only consider the clean estimated indices'''\n",
    "        support_pairs_num, violation_pairs_num = 0, 0\n",
    "        for idx in hypothesis_info_dict['supports']:\n",
    "            if idx not in new_clean_indices:\n",
    "                continue\n",
    "            support_pairs_num += len([idx1 for idx1 in hypothesis_info_dict['supports']\n",
    "                                     [idx] if idx1 in new_clean_indices])\n",
    "\n",
    "        for idx in hypothesis_info_dict['violations']:\n",
    "            if idx not in new_clean_indices:\n",
    "                continue\n",
    "            violation_pairs_num += len(\n",
    "                [idx1 for idx1 in hypothesis_info_dict['violations'][idx] if idx1 in new_clean_indices])\n",
    "\n",
    "        fd_prob = support_pairs_num/(support_pairs_num+violation_pairs_num)\n",
    "\n",
    "        '''Compute mae with previous model value'''\n",
    "        model_mae += abs(new_model[hypothesis]-fd_prob)\n",
    "        new_model[hypothesis] = fd_prob\n",
    "\n",
    "    print(f\"MAE: {model_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_max_num = 1500\n",
    "# dirty_sample_percentage = 0.1\n",
    "\n",
    "# clean_sample_idxs = np.random.choice(list(new_clean_indices), min(len(new_clean_indices), clean_max_num), replace=False)\n",
    "# dirty_sample_idxs = np.random.choice(list(new_dirty_indices), int(dirty_sample_percentage*len(clean_sample_idxs)), replace=False)\n",
    "# # sampled_data_indices = set(clean_sample_idxs).union(set(dirty_sample_idxs))\n",
    "# sampled_data_indices = new_clean_indices.union(new_dirty_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'omdb':{'model':new_model}}\n",
    "model_dict['omdb']['predictions'] =  dict((idx, True) if idx in new_clean_indices else (idx,False) for idx in new_data_indices)\n",
    "with open(\"./trainer_model.json\", 'w') as fp:\n",
    "    json.dump(model_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = raw_df.loc[list(new_data_indices)]\n",
    "# sampled_df['is_clean'] = sampled_df.index.map(lambda x: model_dict['omdb']['predictions'][x])\n",
    "# del sampled_df['is_clean']\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/processed-data\", exist_ok=True)\n",
    "sampled_df.to_csv(\"./data/processed-data/omdb-sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scenarios_dict['omdb']['hypothesis_space']['(title) => director'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Process and Dump pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trainer_model.json', 'r') as f:\n",
    "    models_dict = json.load(f)\n",
    "\n",
    "required_fds = dict(\n",
    "    (scenario, set(models_dict[scenario]['model'].keys())) for scenario in models_dict)\n",
    "\n",
    "with open(\"./data/processed-exp-data/trainer_model.json\", 'w') as fp:\n",
    "    json.dump(models_dict, fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/required_fds.pk\", 'wb') as fp:\n",
    "    pk.dump(required_fds, fp)\n",
    "\n",
    "\n",
    "with open('./new_scenarios.json', 'r') as f:\n",
    "    scenarios = json.load(f)\n",
    "\n",
    "'''Process new_scenarios to make the processing faster later'''\n",
    "processed_df = dict()\n",
    "filtered_processed_scenarios = dict()\n",
    "for dataset in scenarios:\n",
    "\n",
    "    processed_df[dataset] = pd.read_csv(\n",
    "        scenarios['omdb']['processed_dataset_path'], index_col=0)\n",
    "    processed_df[dataset].index = processed_df[dataset].index.map(str)\n",
    "    required_indices = set(processed_df[dataset].index)\n",
    "\n",
    "    filtered_processed_scenarios= {dataset:{'data_indices': set(\n",
    "        scenarios[dataset]['data_indices']).intersection(required_indices), 'hypothesis_space': dict()}}\n",
    "\n",
    "    '''Filter required fds and data_indices'''\n",
    "    for hypothesis in tqdm(scenarios[dataset]['hypothesis_space']):\n",
    "        if hypothesis not in required_fds[dataset]:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis]={'lfd':set(\n",
    "            scenarios[dataset]['hypothesis_space'][hypothesis]['lfd']),\n",
    "        'rfd': set(\n",
    "            scenarios[dataset]['hypothesis_space'][hypothesis]['rfd'])}\n",
    "\n",
    "        for info_type in ['supports', 'violations']:\n",
    "            filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type] = dict()\n",
    "\n",
    "            filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][f'{info_type[:-1]}_pairs'] = set()\n",
    "            for idx in scenarios[dataset]['hypothesis_space'][hypothesis][info_type]:\n",
    "                if idx not in required_indices:\n",
    "                    continue\n",
    "\n",
    "                filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx] = set(\n",
    "                    scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx]).intersection(required_indices)\n",
    "                \n",
    "                pairs = set((idx, idx_) if idx<idx_ else (idx_, idx) for idx_ in filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][info_type][idx])\n",
    "                filtered_processed_scenarios[dataset]['hypothesis_space'][hypothesis][f'{info_type[:-1]}_pairs'] |= pairs\n",
    "\n",
    "\n",
    "with open(\"./data/processed-exp-data/filtered_processed_scenarios.pk\", 'wb') as fp:\n",
    "    pk.dump(filtered_processed_scenarios, fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/processed_dfs.pk\", 'wb') as fp:\n",
    "    pk.dump(processed_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_processed_scenarios['omdb'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pk\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trainer_model.json', 'r') as f:\n",
    "    _models_dict = json.load(f)\n",
    "\n",
    "with open(\"./data/processed-exp-data/filtered_processed_scenarios.pk\", 'rb') as fp:\n",
    "    _filtered_processed_scenarios = pk.load(fp)\n",
    "\n",
    "with open(\"./data/processed-exp-data/processed_dfs.pk\", 'rb') as fp:\n",
    "    _processed_df = pk.load(fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_clean_prob(idx, fd, fd_prob, scenario_id, data_indices=None):\n",
    "    if data_indices is None:\n",
    "        compliance_num = len(\n",
    "            _filtered_processed_scenarios[scenario_id]['hypothesis_space'][fd]['supports'].get(idx, []))\n",
    "        violation_num = len(\n",
    "            _filtered_processed_scenarios[scenario_id]['hypothesis_space'][fd]['violations'].get(idx, []))\n",
    "    else:\n",
    "        compliance_num = len([idx_ for idx_ in _filtered_processed_scenarios[scenario_id]['hypothesis_space']\n",
    "                                [fd]['supports'].get(idx, [])\n",
    "                                if idx_ in data_indices])\n",
    "        violation_num = len([idx_ for idx_ in _filtered_processed_scenarios[scenario_id]['hypothesis_space']\n",
    "                            [fd]['violations'].get(idx, []) if idx_ in data_indices])\n",
    "\n",
    "    tuple_clean_score = math.exp(fd_prob*(compliance_num-violation_num))\n",
    "    tuple_dirty_score = math.exp(fd_prob*(-compliance_num+violation_num))\n",
    "    cond_p_clean = tuple_clean_score/(tuple_clean_score+tuple_dirty_score)\n",
    "\n",
    "    return cond_p_clean\n",
    "\n",
    "def get_average_cond_clean_prediction(indices, model, scenario_id):\n",
    "    conditional_clean_probability_dict = dict()\n",
    "    indices = set(indices)\n",
    "    for idx in indices:\n",
    "        cond_clean_prob = mean([compute_conditional_clean_prob(\n",
    "            idx=idx, fd=fd, fd_prob=fd_prob, scenario_id=scenario_id,\n",
    "            data_indices=indices) for fd, fd_prob in model.items()])  # whether to include the validation_indices or all the data_indices while computing the conditional clean probability\n",
    "        conditional_clean_probability_dict[idx] = cond_clean_prob\n",
    "    return conditional_clean_probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_model = dict(sorted(_models_dict['omdb']['model'].items(), key=itemgetter(1),\n",
    "                    reverse=True)[:10])\n",
    "_clean_indices = set([idx for idx in _models_dict['omdb']['predictions'] if _models_dict['omdb']['predictions'][idx]])\n",
    "_clean_probab_dict = get_average_cond_clean_prediction(_processed_df['omdb'].index, model=_model, scenario_id='omdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the clean label in the model file\n",
    "for idx in _processed_df['omdb'].index:\n",
    "    _clean = _clean_probab_dict[idx] >= 0.5\n",
    "    if _clean == _models_dict['omdb']['predictions'][idx]:\n",
    "        continue\n",
    "    print(idx, _clean, _models_dict['omdb']['predictions'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the aggreage model on the overall data\n",
    "for hypothesis in _models_dict['omdb']['model']:\n",
    "    hypothesis_info_dict = _filtered_processed_scenarios['omdb']['hypothesis_space'][hypothesis]\n",
    "\n",
    "    support_pairs_num, violation_pairs_num = 0,0\n",
    "    for idx in hypothesis_info_dict['supports']:\n",
    "        if idx not in _clean_indices:\n",
    "            continue\n",
    "        support_pairs_num += len(set(hypothesis_info_dict['supports'][idx]).intersection(_clean_indices))\n",
    "\n",
    "    for idx in hypothesis_info_dict['violations']:\n",
    "        if idx not in _clean_indices:\n",
    "            continue\n",
    "        violation_pairs_num += len(set(hypothesis_info_dict['violations'][idx]).intersection(_clean_indices))\n",
    "    \n",
    "    is_correct = (support_pairs_num/(support_pairs_num+violation_pairs_num)) == _models_dict['omdb']['model'][hypothesis]\n",
    "    \n",
    "    if not is_correct:\n",
    "        print((support_pairs_num/(support_pairs_num+violation_pairs_num)),  _models_dict['omdb']['model'][hypothesis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "validation_indices = {'omdb': sample(list(_models_dict['omdb']['predictions'].keys()), 1000)}\n",
    "\n",
    "with open('./data/processed-data/validation_indices.json','w') as fp:\n",
    "    json.dump(validation_indices, fp)\n",
    "\n",
    "validation_indices['omdb'] = set(validation_indices['omdb'])\n",
    "\n",
    "with open('./data/processed-exp-data/validation_indices.pk','wb') as fp:\n",
    "    pk.dump(validation_indices, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebe342ebb4b42c195cb24ceb851d2936317252a8529c8b64c2907583366acbf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
